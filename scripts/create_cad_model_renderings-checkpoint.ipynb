{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "def fibonacci_sphere(samples=1):\n",
    "    points = []\n",
    "    phi = math.pi * (3. - math.sqrt(5.))  # golden angle in radians\n",
    "\n",
    "    for i in range(samples):\n",
    "        y = 1 - (i / float(samples - 1)) * 2  # y goes from 1 to -1\n",
    "        radius = math.sqrt(1 - y * y)  # radius at y\n",
    "\n",
    "        theta = phi * i  # golden angle increment\n",
    "\n",
    "        x = math.cos(theta) * radius\n",
    "        z = math.sin(theta) * radius\n",
    "\n",
    "        points.append((x, y, z))\n",
    "    return points\n",
    "\n",
    "def rotation_matrix_from_vectors(vec1, vec2):\n",
    "    \"\"\" Find the rotation matrix that aligns vec1 to vec2\n",
    "    :param vec1: A 3d \"source\" vector\n",
    "    :param vec2: A 3d \"destination\" vector\n",
    "    :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.\n",
    "    \"\"\"\n",
    "    a, b = (vec1 / np.linalg.norm(vec1)).reshape(3), (vec2 / np.linalg.norm(vec2)).reshape(3)\n",
    "    v = np.cross(a, b)\n",
    "    c = np.dot(a, b)\n",
    "    s = np.linalg.norm(v)\n",
    "    kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "    rotation_matrix = np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2))\n",
    "    return rotation_matrix\n",
    "\n",
    "    \n",
    "    .\n",
    "    \n",
    "    \n",
    "def get_mat(sample):\n",
    "    mat = []\n",
    "    \n",
    "    _nr_points = sample\n",
    "    phi = np.arange(-np.pi/2, np.pi/2 , np.pi / sample )\n",
    "    \n",
    "    \n",
    "    for i in range (0,len(phi)):\n",
    "        r = math.cos(phi[i])\n",
    "        \n",
    "        #2 pi -> \n",
    "        #print(r, \"step size\", 1/ (r* 2 * np.pi / sample))\n",
    "        _nr_points = int( r * sample )\n",
    "        if _nr_points < 1:\n",
    "            step_size =  2 * np.pi\n",
    "        else:\n",
    "            step_size =  (2 * np.pi) / _nr_points \n",
    "        \n",
    "        \n",
    "        #print(step_size)\n",
    "        theta = np.arange(0, 2 * np.pi , step_size )\n",
    "        print(phi[i], \"rad\", r, len(theta), \"step size\", step_size )\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range (0,len(theta)):\n",
    "            mat.append( R.from_euler('XZY',[phi[i],theta[j],0]).as_matrix() ) \n",
    "            print (theta[j])\n",
    "    \n",
    "    return np.array(mat)\n",
    "\n",
    "    \n",
    "mats = get_mat(sample=30)\n",
    "mats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "a = math.sqrt(1/3)\n",
    "c = math.sqrt(a*a + a*a +a*a )\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k3d\n",
    "import numpy as np\n",
    "\n",
    "# def get_mat(nr_points):\n",
    "#     points = fibonacci_sphere(nr_points)\n",
    "#     mat = []\n",
    "#     for i in range(0, nr_points):\n",
    "#         mat.append(rotation_matrix_from_vectors(np.array([math.sqrt(1/3),math.sqrt(1/3),math.sqrt(1/3)]),np.array(points[i])))\n",
    "#     return np.array(mat)\n",
    "\n",
    "\n",
    "# nr_points = 30\n",
    "# points = fibonacci_sphere(nr_points)\n",
    "# mat = []\n",
    "# for i in range(0, nr_points):\n",
    "#     mat.append(rotation_matrix_from_vectors(np.array([math.sqrt(1/3),math.sqrt(1/3),math.sqrt(1/3)]),np.array(points[i])))\n",
    "# mat = np.array(mat)\n",
    "\n",
    "\n",
    "\n",
    "mat = get_mat(60)\n",
    "nr_points = mat.shape[0]\n",
    "print(nr_points)\n",
    "# validateing matrix by plotting the camera viewpoints\n",
    "unit_point = np.ones( (nr_points,3) )\n",
    "#unit_point[:,0] = 1\n",
    "\n",
    "for i in range(0,nr_points):\n",
    "    unit_point[i,:] =  unit_point[i,:].dot( mat[i,:,:].T )\n",
    "    \n",
    "\n",
    "plot = k3d.plot(name='points')\n",
    "\n",
    "points = unit_point.tolist()\n",
    "\n",
    "point_size = 0.05\n",
    "x_rgb = (0,0,255)\n",
    "x_col = []\n",
    "for i in range (0, len(points )):\n",
    "    rgb_int = int('%02x%02x%02x' % x_rgb, 16)\n",
    "    x_col.append( rgb_int ) \n",
    "\n",
    "plt_points = k3d.points(points, np.array(x_col).astype(np.uint32), point_size=point_size)\n",
    "plot += plt_points\n",
    "\n",
    "plt_points.shader='3d'\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "import pyglet\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import pyrender\n",
    "from math import pi\n",
    "from PIL import Image\n",
    "import copy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import scipy.io as scio\n",
    "import pickle as pkl\n",
    "import time\n",
    "import imageio\n",
    "#same as ycb first go and synthetic data of ycb\n",
    "cx = 312.9869\n",
    "cy = 241.3109\n",
    "fx = 1066.778\n",
    "fy = 1067.487\n",
    "\n",
    "obj = '005_tomato_soup_can'#'005_tomato_soup_can'\n",
    "obj_idx_tomoto_soup = 4\n",
    "\n",
    "model = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/models'\n",
    "base = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/data/0003'\n",
    "desig = '000010'\n",
    "store = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/viewpoints_renderings'\n",
    "\n",
    "img = Image.open('{0}/{1}-color.png'.format(base, desig))\n",
    "depth = Image.open('{0}/{1}-depth.png'.format(base, desig))\n",
    "meta = scio.loadmat('{0}/{1}-meta.mat'.format(base, desig))\n",
    "\n",
    "obj_tmp = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "obj_idx_in_list = int(np.argwhere(obj_tmp == obj_idx_tomoto_soup))\n",
    "target_r = np.array(meta['poses'][:, :, obj_idx_in_list][:, 0:3])\n",
    "target_t = np.array(\n",
    "    [meta['poses'][:, :, obj_idx_in_list][:, 3:4].flatten()])[0,:]\n",
    "\n",
    "\n",
    "def render_obj(obj, model, store, idx, obj_dis, r_mat,fx, fy, cx, cy,):\n",
    "    ### params\n",
    "    w = 640\n",
    "    h = 480\n",
    "\n",
    "    path = os.path.join(store, obj)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    obj_mesh = trimesh.load(f'{model}/{obj}/textured.obj')\n",
    "    mesh = pyrender.Mesh.from_trimesh(obj_mesh, smooth=True, wireframe=False)\n",
    "    scene = pyrender.Scene(bg_color=(0, 0, 0, 255))\n",
    "    \n",
    "    pose_obj = np.eye(4)\n",
    "    pose_obj[:3, 3] = [0, 0, obj_dis]\n",
    "    pose_obj[:3, :3] = r_mat\n",
    "\n",
    "    scene.add(mesh, pose=copy.copy(pose_obj))\n",
    "    ren = pyrender.OffscreenRenderer(w, h, point_size=1)\n",
    "\n",
    "\n",
    "    \n",
    "    camera = pyrender.IntrinsicsCamera(\n",
    "        fx, fy, cx, cy, znear=0.1, zfar=2, name=None)\n",
    "    camera_pose = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0.0, 0],\n",
    "        [0.0, 0, 1, 0],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ])\n",
    "    camera_pose[:3,:3] = R.from_euler('xyz',[0,180,180], degrees=True).as_matrix()\n",
    "    scene.add(camera, pose=camera_pose)\n",
    "    light = pyrender.SpotLight(color=np.ones(3), intensity=25.0,\n",
    "                               innerConeAngle=np.pi * 0.1)\n",
    "    scene.add(light, pose=camera_pose)\n",
    "    color, depth = ren.render(scene)\n",
    "    color = color[:,:,[2,1,0]] #change to rgb \n",
    "    cv2.imwrite(f'{path}/{idx}.png',color)\n",
    "    print(depth)\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(depth)\n",
    "    plt.axis(\"off\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    color = color[:,:,[2,1,0]]\n",
    "    plt.imshow(color)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return color,depth\n",
    "\n",
    "class RenderSingleObj():\n",
    "    def __init__(self,obj, model, store,fx, fy, cx, cy, w, h):\n",
    "\n",
    "        self.path = os.path.join(store, obj)\n",
    "        if not os.path.exists(self.path):\n",
    "            os.mkdir(self.path)\n",
    "            \n",
    "            \n",
    "        obj_mesh = trimesh.load(f'{model}/{obj}/textured.obj')\n",
    "        self.mesh = pyrender.Mesh.from_trimesh(obj_mesh, smooth=True, wireframe=False)\n",
    "        \n",
    "        self.scene = pyrender.Scene(bg_color=(0, 0, 0, 255))\n",
    "        \n",
    "        self.ren = pyrender.OffscreenRenderer(w, h, point_size=1)\n",
    "\n",
    "\n",
    "\n",
    "        camera = pyrender.IntrinsicsCamera(\n",
    "            fx, fy, cx, cy, znear=0.1, zfar=2, name=None)\n",
    "        camera_pose = np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0.0, 0],\n",
    "            [0.0, 0, 1, 0],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "        ])\n",
    "        camera_pose[:3,:3] = R.from_euler('xyz',[0,180,180], degrees=True).as_matrix()\n",
    "        self.scene.add(camera, pose=camera_pose)\n",
    "        light = pyrender.SpotLight(color=np.ones(3), intensity=25.0,\n",
    "                                   innerConeAngle=np.pi * 0.1)\n",
    "        self.scene.add(light, pose=camera_pose)\n",
    "        \n",
    "        pose_obj = np.eye(4)\n",
    "        self.obj_node = self.scene.add(self.mesh, pose=pose_obj)\n",
    "        \n",
    "    def render(self, idx, obj_dis , r_mat, plot):\n",
    "        \n",
    "        pose_obj = np.eye(4)\n",
    "        pose_obj[:3, 3] = [0, 0, obj_dis]\n",
    "        pose_obj[:3, :3] = r_mat\n",
    "        \n",
    "        self.scene.set_pose(self.obj_node, pose=pose_obj)\n",
    "        \n",
    "        color, depth = self.ren.render(self.scene)\n",
    "        color = color[:,:,[2,1,0]] #change to rgb \n",
    "        cv2.imwrite(f'{self.path}/{idx}-color.png',color)\n",
    "        \n",
    "        depth_store = np.array(depth * 10000, dtype=np.uint16)\n",
    "        imageio.imwrite(f'{self.path}/{idx}-depth.png', depth_store)\n",
    "        \n",
    "        if np.amax(depth_store[0,:]) != 0 or \\\n",
    "            np.amax(depth_store[-1,:]) != 0 or \\\n",
    "            np.amax(depth_store[:,0]) != 0 or \\\n",
    "            np.amax(depth_store[:,-1]) != 0:\n",
    "            print(\"ERRRORORRR RUN obj again\")\n",
    "            return None, None\n",
    "            \n",
    "                \n",
    "        if plot:\n",
    "            fig = plt.figure()\n",
    "            fig.add_subplot(1, 2, 1)\n",
    "            plt.imshow(depth)\n",
    "            plt.axis(\"off\")\n",
    "            fig.add_subplot(1, 2, 2)\n",
    "            color = color[:,:,[2,1,0]]\n",
    "            plt.imshow(color)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        \n",
    "        return color,depth\n",
    "\n",
    "\n",
    "print(generic._backend._name_to_idx)\n",
    "obj_names = list(generic._backend._name_to_idx.keys())\n",
    "obj_scale = [0.4]* len(obj_names)\n",
    "print(obj_scale)\n",
    "\n",
    "for j, obj in enumerate(obj_names):\n",
    "    renSingObj = RenderSingleObj(obj, model, store,fx, fy, cx, cy, w=640, h=480 )\n",
    "    broken = False\n",
    "    print(\"start\", obj)\n",
    "    while 1:\n",
    "        if broken:\n",
    "            obj_scale[j] += 0.1  \n",
    "            print(\"incresed\", obj, \" to \",obj_scale[j] )\n",
    "        \n",
    "        # try to render the object with the set distance\n",
    "        broken = False\n",
    "        \n",
    "        for i in range(nr_points): #nr_points\n",
    "            start = time.time()\n",
    "\n",
    "            res, res2 = renSingObj.render(idx=i, obj_dis=obj_scale[j] , r_mat= mat[i] , plot= False)\n",
    "            delta = time.time() - start\n",
    "            #print(f'Progress: {i}/{nr_points}, Estimated time: {(nr_points-i)*delta}s, fps: {delta} s')\n",
    "            if res is None:\n",
    "                print(\"broken\")\n",
    "                broken = True\n",
    "                break\n",
    "        if not broken:\n",
    "            print(\"Obs was successfull ended up useing\", obj_scale[j])\n",
    "            break\n",
    "\n",
    "obj_names_repeat= ['002_master_chef_can','019_pitcher_base','021_bleach_cleanser']\n",
    "# these 3 object fit to tight. Therfore increase the bb. \n",
    "# they work with 30 points but dont wih 500 (this is expected)\n",
    "obj_scale[10] = 0.8\n",
    "obj_scale[11] = 0.7\n",
    "obj_scale[0] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is obj_scale and obj_names\n",
    "\n",
    "nr_points = 500\n",
    "# comute camera matrix for nr_points\n",
    "from scipy.stats import special_ortho_group\n",
    "\n",
    "mat = special_ortho_group.rvs(dim = 3, size = nr_points)\n",
    "\n",
    "# best way to store cam cal use matrix defined here\n",
    "# https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\n",
    "cam_mat = np.zeros( (nr_points,3,3) )\n",
    "cam_mat[:,2,2] = 1\n",
    "cam_mat[:,0,0] = fx\n",
    "cam_mat[:,1,1] = fy\n",
    "cam_mat[:,0,2] = cx\n",
    "cam_mat[:,1,2] = cy\n",
    "\n",
    "\n",
    "# render all objects with 500 viewpoints each\n",
    "\n",
    "for j, obj in enumerate(obj_names):\n",
    "    print(\"Start\", obj)\n",
    "    \n",
    "    # creat renderer\n",
    "    renSingObj = RenderSingleObj(obj, model, store,fx, fy, cx, cy, w=640, h=480 )\n",
    "    \n",
    "    # store cam pose for each frame\n",
    "    homo = np.repeat(np.eye(4).reshape((1,4,4)),nr_points, axis=0)\n",
    "    homo[:,:3,:3] = np.array(mat)\n",
    "    homo[:,:3,3] = [0,0,obj_scale[j]]\n",
    "    \n",
    "    # dump pose and cam_calibration matrix\n",
    "    pkl.dump( homo, open( f'{store}/{obj}/pose.pkl', \"wb\" ) )\n",
    "    pkl.dump( cam_mat, open( f'{store}/{obj}/cam.pkl', \"wb\" ) )\n",
    "    \n",
    "    \n",
    "    # render frames\n",
    "    for i in range(nr_points): #nr_points\n",
    "        start = time.time()\n",
    "        res, res2 = renSingObj.render(idx=i, obj_dis=obj_scale[j] , r_mat= mat[i] , plot= False)\n",
    "        delta = time.time() - start\n",
    "        if i % 50 == 1:\n",
    "            print(f'Progress: {i}/{nr_points}, Estimated time: {(nr_points-i)*delta}s, fps: {delta} s')\n",
    "        if res is None:\n",
    "            print(\"ERRRRRRRRRRROOOOOOOOOOORRRRR\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test of hyper_params\n",
    "test = pkl.load( open( f'{store}/{obj}/pose.pkl', \"rb\" ) )\n",
    "print(f'{store}/{obj}/pose.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used to verify the generated depth map\n",
    "\n",
    "from PIL import Image\n",
    "base = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/data/0003'\n",
    "desig = '000010'\n",
    "obj_idx_tomoto_soup = 4\n",
    "\n",
    "img = Image.open('{0}/{1}-color.png'.format(base, desig))\n",
    "depth = Image.open('{0}/{1}-depth.png'.format(base, desig))\n",
    "meta = scio.loadmat('{0}/{1}-meta.mat'.format(base, desig))\n",
    "\n",
    "\n",
    "\n",
    "depth_ren = Image.open('/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/viewpoints_renderings/005_tomato_soup_can/1-depth.png')\n",
    "\n",
    "#np.amax(render_depth*10000)\n",
    "#np.amin(depth)\n",
    "np.amin(depth)\n",
    "\n",
    "depth\n",
    "print(type(depth))\n",
    "d_arr = np.array(depth)\n",
    "d_r_arr = np.array(depth_ren)\n",
    "print(d_r_arr[100:200,100:200] )\n",
    "\n",
    "depth\n",
    "print( np.amax(d_arr[200:280, 250:350]), np.amin(d_arr[200:280, 250:350]) ,  np.amax( d_r_arr[200:280, 250:350] ), np.amin(d_r_arr[200:280, 250:350])  )\n",
    "depth_ren\n",
    "\n",
    "\n",
    "depth_masked = d_r_arr.flatten()[:, np.newaxis].astype(np.float32)\n",
    "xmap = np.array([[j for i in range(640)] for j in range(480)])\n",
    "ymap = np.array([[i for i in range(640)] for j in range(480)])\n",
    "xmap_masked = xmap.flatten()[:, np.newaxis].astype(np.float32)\n",
    "ymap_masked = ymap.flatten()[:, np.newaxis].astype(np.float32)\n",
    "\n",
    "cam_scale = meta['factor_depth'][0][0]\n",
    "print(cam_scale)\n",
    "\n",
    "pt2 = depth_masked / cam_scale\n",
    "pt0 = (ymap_masked - cx) * pt2 / fx\n",
    "pt1 = (xmap_masked - cy) * pt2 / fy\n",
    "cloud = np.concatenate((pt0, pt1, pt2), axis=1)\n",
    "\n",
    "print(np.amax(cloud[:,2]))\n",
    "print(np.amin(cloud[:,2]))\n",
    "depth_ren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now go and use the created depth map with the knowledge about the rotation to mimic an orginal image \n",
    "# for testing load orginal image \n",
    "# load gt transformation\n",
    "\n",
    "from PIL import Image\n",
    "base = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/data/0003'\n",
    "desig = '000010'\n",
    "obj_idx_tomoto_soup = 4\n",
    "\n",
    "img = Image.open('{0}/{1}-color.png'.format(base, desig))\n",
    "depth = Image.open('{0}/{1}-depth.png'.format(base, desig))\n",
    "meta = scio.loadmat('{0}/{1}-meta.mat'.format(base, desig))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(depth)\n",
    "plt.axis(\"off\")\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "obj = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "obj_idx_in_list = int(np.argwhere(obj == obj_idx_tomoto_soup))\n",
    "target_r = np.array(meta['poses'][:, :, obj_idx_in_list][:, 0:3])\n",
    "target_t = np.array(\n",
    "    [meta['poses'][:, :, obj_idx_in_list][:, 3:4].flatten()])[0,:]\n",
    "\n",
    "\n",
    "obj = '005_tomato_soup_can'\n",
    "model = '/media/scratch1/jonfrey/datasets/YCB_Video_Dataset/models'\n",
    "\n",
    "render_img,render_depth = render_obj(obj, model, base,idx=0 , obj_dis=0.4 , r_mat= target_r )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rendered object is now same as in real image. \n",
    "# next problem for offline render compute the correct scale and translation of the object in the camera coordinates.\n",
    "img_r = copy.deepcopy(render_img)\n",
    "img_ori = np.array(copy.deepcopy(img))\n",
    "# current position of the object is \n",
    "render_t = [0,0,obj_dis]\n",
    "target_t\n",
    "\n",
    "\n",
    "def backproject_point(p, fx,fy,cx,cy):\n",
    "    u = int(((p[0] / p[2]) * fx) + cx)\n",
    "    v = int(((p[1] / p[2]) * fy) + cy)\n",
    "    return u,v \n",
    "\n",
    "def backproject_points(p, fx,fy,cx,cy):\n",
    "    \"\"\"\n",
    "    p.shape = (nr_points,xyz)\n",
    "    \"\"\"\n",
    "    u = np.round(( np.true_divide( p[:,0] , p[:,2]) * fx) + cx).astype(np.int32)\n",
    "    v = np.round(( np.true_divide( p[:,1] , p[:,2]) * fy) + cy).astype(np.int32)\n",
    "    return np.stack([v,u]).T\n",
    "\n",
    "\n",
    "#verify that backprojection of CenterOfObject works\n",
    "u,v = backproject_point(render_t, fx,fy,cx,cy)\n",
    "print(u,v)\n",
    "d = 5\n",
    "img_r[v-d:v+d,u-d:u+d,:] = [0,255,0]\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(img_r)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(target_t)\n",
    "u,v = backproject_point(target_t, fx,fy,cx,cy)\n",
    "img_ori[v-d:v+d,u-d:u+d,:] = [0,255,0]\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(img_ori )\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next load cad_model\n",
    "# sample two points with minimal distance wenn projected to the rendered image! \n",
    "# project the two points and get the scaleing transformation. \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('/home/jonfrey/PLR')\n",
    "sys.path.append('src')\n",
    "sys.path.append('src/dense_fusion')\n",
    "\n",
    "from loaders_v2 import Backend, ConfigLoader, GenericDataset\n",
    "from visu import Visualizer\n",
    "from PIL import Image\n",
    "import copy\n",
    "from helper import re_quat\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "exp_cfg = ConfigLoader().from_file(\n",
    "    '/home/jonfrey/PLR/src/loaders_v2/test/dataset_cfgs.yml')\n",
    "env_cfg = ConfigLoader().from_file(\n",
    "    '/home/jonfrey/PLR/src/loaders_v2/test/env_ws.yml')\n",
    "\n",
    "generic = GenericDataset(\n",
    "    cfg_d=exp_cfg['d_ycb'],\n",
    "    cfg_env=env_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Checking points for real image: \n",
    "model_points = generic._backend._pcd_cad_dict[obj_idx_tomoto_soup]\n",
    "target = np.dot(model_points, target_r.T)\n",
    "target = np.add(target, target_t)\n",
    "\n",
    "# select first point:\n",
    "a = 0\n",
    "\n",
    "# select secound point with maximum distance to a \n",
    "pts = target.shape[0]\n",
    "\n",
    "a_= np.reshape(target[a,:],(1,3))\n",
    "a_ =np.repeat(a_,pts, axis=0) \n",
    "dis = np.linalg.norm(target-a_ ,axis=1)\n",
    "b = np.argmax(dis)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line (Another vector formulation)\n",
    "# select third point with maximum distance to line a->b\n",
    "u = target[a,:]-target[b,:]\n",
    "dis2 = []\n",
    "for i in range(pts):\n",
    "    nen = np.linalg.norm(np.cross(target[i,:]-target[a,:],u))\n",
    "    denom = np.linalg.norm( u )\n",
    "    dis_tmp = np.true_divide(nen, denom)\n",
    "    dis2.append(dis_tmp)\n",
    "dis2 = np.array(dis2)\n",
    "c = np.argmax(dis2)\n",
    "\n",
    "print(\"Selected points\",a,b,c)\n",
    "\n",
    "target_pts= target[(a,b,c),:]\n",
    "print(target_pts)\n",
    "visu = Visualizer(None,'/')\n",
    "visu.plot_estimated_pose('Target%s' % i, 1, copy.deepcopy(img_ori), target_pts, np.zeros((1, 3)), np.eye(3),\n",
    "    cam_cx=cx,\n",
    "    cam_cy=cy,\n",
    "    cam_fx=fx,\n",
    "    cam_fy=fy,\n",
    "    store=False,\n",
    "    jupyter=True, w=1)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking points for real image: \n",
    "model_points = generic._backend._pcd_cad_dict[obj_idx_tomoto_soup]\n",
    "target = np.dot(model_points, target_r.T)\n",
    "target = np.add(target, target_t)\n",
    "\n",
    "syn_r = target_r\n",
    "syn_t = np.array([0,0,obj_dis])\n",
    "model_points = generic._backend._pcd_cad_dict[obj_idx_tomoto_soup]\n",
    "syn_target = np.dot(model_points, syn_r.T)\n",
    "syn_target = np.add(syn_target, syn_t)\n",
    "\n",
    "syn_target_pts= syn_target[(a,b,c),:]\n",
    "\n",
    "\n",
    "# visu = Visualizer(None,'/')\n",
    "# visu.plot_estimated_pose('Target%s' % i, 1, copy.deepcopy(img_r), syn_target_pts, np.zeros((1, 3)), np.eye(3),\n",
    "#     cam_cx=cx,\n",
    "#     cam_cy=cy,\n",
    "#     cam_fx=fx,\n",
    "#     cam_fy=fy,\n",
    "#     store=False,\n",
    "#     jupyter=True, w=1)\n",
    "# print(fx,fy,cx,cy)\n",
    "# print(syn_target_pts)\n",
    "\n",
    "\n",
    "# render_pixles = backproject_points(syn_target_pts, fx,fy,cx,cy)\n",
    "# orig_pixles = backproject_points(target[(a,b,c),:], fx,fy,cx,cy)\n",
    "\n",
    "# print(render_pixles)\n",
    "# print(orig_pixles)\n",
    "\n",
    "# with these pixel we look for scale trans_u , trans_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bb():\n",
    "    def __init__(self,p1, p2):\n",
    "        \"p1 = u,v  u=height v=widht starting top_left 0,0\"\n",
    "        if p1[0] < p2[0] and p1[1] < p2[1]:\n",
    "            print(\"p1 = top_left\")\n",
    "            self.tl = p1\n",
    "            self.br = p2\n",
    "        elif p1[0] > p2[0] and p1[1] > p2[1]:\n",
    "            print(\"p1 = bottom_right\")\n",
    "            self.br = p1\n",
    "            self.tl = p2\n",
    "        elif p1[0] > p2[0] and p1[1] < p2[1]:\n",
    "            print(\"p1 = bottom_left\")\n",
    "            self.tl = copy.copy(p1)\n",
    "            self.tl[0] =p2[0]\n",
    "            self.br = p2\n",
    "            self.br[0] = p1[0]\n",
    "        else:\n",
    "            print(\"p1 = top_right\")\n",
    "            self.br = copy.copy(p1)\n",
    "            self.br[0] = p2[0]\n",
    "            self.tl = p2\n",
    "            self.tl[0] = p1[0]\n",
    "            \n",
    "    def __str__(self):\n",
    "        w = self.width()\n",
    "        h = self.height()\n",
    "        return f'TL Cor: {self.tl}, BR Cor: {self.br}, Widht: {w}, Height: {h}'\n",
    "    def width(self):\n",
    "\n",
    "        return (self.br[1] - self.tl[1])\n",
    "    def height(self):\n",
    "        return (self.br[0] - self.tl[0])\n",
    "    def move (self,u,v):\n",
    "        self.br[0] += u\n",
    "        self.tl[0] += u\n",
    "        self.br[1] += v\n",
    "        self.tl[1] += v\n",
    "    def expand(self, r):\n",
    "        r = r - 1 \n",
    "        self.br[0] = int(self.br[0]+ self.height()*r)\n",
    "        self.tl[0] = int(self.tl[0]- self.height()*r)\n",
    "        self.br[1] = int(self.br[1]+ self.height()*r)\n",
    "        self.tl[1] = int(self.tl[1]- self.height()*r)\n",
    "    def add_margin(self, u,v):\n",
    "        self.br[0] += u\n",
    "        self.tl[0] -= u\n",
    "        self.br[1] += v\n",
    "        self.tl[1] -= v\n",
    "    def limit_bb(self,max_height, max_width):\n",
    "        if self.tl[0] < 0:\n",
    "            self.tl[0] = 0\n",
    "        elif self.tl[0] > max_height:\n",
    "            self.tl[0] = max_height\n",
    "        if self.br[0] < 0:\n",
    "            self.br[0] = 0\n",
    "        elif self.br[0] > max_height:\n",
    "            self.br[0] = max_height\n",
    "        if self.tl[1] < 0:\n",
    "            self.tl[1] = 0\n",
    "        elif self.tl[1] > max_width:\n",
    "            self.tl[1] = max_width\n",
    "        if self.br[1] < 0:\n",
    "            self.br[1] = 0\n",
    "        elif self.br[1] > max_width:\n",
    "            self.br[1] = max_width\n",
    "    def crop(self,img):\n",
    "        return img[self.tl[0]:self.br[0],self.tl[1]:self.br[1],:]\n",
    "    def add_noise(self, std_h, std_w):\n",
    "        # std_h is the variance that is added to the top corrner position and, bottom_corner position\n",
    "        self.br = np.random.normal(self.br, np.array( [std_h,std_w])  ).astype(dtype=np.int32)\n",
    "        self.tl = np.random.normal(self.tl, np.array( [std_h,std_w])  ).astype(dtype=np.int32)\n",
    "    def expand_to_correct_ratio(self, w,h):\n",
    "        if self.width()/self.height() > w/h:\n",
    "            scale_ratio = h/self.height()\n",
    "            h_set = self.width()*(h/w)\n",
    "            add_w = 0\n",
    "            add_h = int((h_set-self.height())/2)\n",
    "        else:\n",
    "            scale_ratio = h/self.height()\n",
    "            w_set = self.height()*(w/h)\n",
    "            add_h = 0\n",
    "            add_w = int((w_set-self.width())/2)\n",
    "\n",
    "        self.add_margin(add_h,add_w)\n",
    "    def plot(self, img, w=5):\n",
    "        test = copy.deepcopy(img)\n",
    "        w = 5\n",
    "        test[self.tl[0]:self.br[0], self.tl[1]-w : self.tl[1]+w ] = [0,255,0]\n",
    "        test[self.tl[0]:self.br[0], self.br[1]-w : self.br[1]+w ] = [0,255,0]\n",
    "        \n",
    "        test[self.tl[0]-w:self.tl[0]+w, self.tl[1] : self.br[1] ] = [0,255,0]\n",
    "        test[self.br[0]-w:self.br[0]+w, self.tl[1] : self.br[1] ] = [0,255,0]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig.add_subplot(1, 1, 1)\n",
    "        plt.imshow(test)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "p1 = [10,130]\n",
    "p2 = [20,120]\n",
    "test_bb = bb(p1,p2)\n",
    "print(test_bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ren_pixles = backproject_points(syn_target, fx,fy,cx,cy)\n",
    "all_orig_pixles = backproject_points(target, fx,fy,cx,cy)\n",
    "    \n",
    "    \n",
    "def get_bb( pixles):\n",
    "    idx_b_r =(np.argmax(pixles[:,0]),np.argmax(pixles[:,1]))\n",
    "    idx_t_l =(np.argmin(pixles[:,0]),np.argmin(pixles[:,1]))\n",
    "    b_r = [pixles[idx_b_r[0],0],pixles[idx_b_r[1],1]]\n",
    "    t_l = [pixles[idx_t_l[0],0],pixles[idx_t_l[1],1]]\n",
    "    \n",
    "    return bb(b_r, t_l)\n",
    "def verify(img, bb):\n",
    "    test = copy.deepcopy(img)\n",
    "    test[bb.tl[0]:bb.tl[0]+5,bb.tl[1]:bb.tl[1]+5] = [0,255,0]\n",
    "    test[bb.br[0]:bb.br[0]+5,bb.br[1]:bb.br[1]+5] = [0,255,0]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(1, 1, 1)\n",
    "    plt.imshow(test)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "def plt_img(img):\n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(1, 1, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "bb_ren = get_bb(all_ren_pixles)\n",
    "bb_orig = get_bb(all_orig_pixles)\n",
    "\n",
    "w = 640\n",
    "h = 480\n",
    "center_ren = backproject_point(render_t, fx,fy,cx,cy)\n",
    "center_orig = backproject_point(target_t, fx,fy,cx,cy)\n",
    "\n",
    "\n",
    "bb_orig_cen = copy.deepcopy(bb_orig)\n",
    "bb_orig_cen.move(-center_orig[0],-center_orig[1])\n",
    "\n",
    "bb_ren_cen = copy.deepcopy(bb_ren)\n",
    "bb_ren_cen.move(-center_ren[0],-center_ren[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "#make a deep copy so the cells are independet\n",
    "bb_ren = copy.deepcopy(bb_ren_cen)\n",
    "bb_orig = copy.deepcopy(bb_orig_cen)\n",
    "\n",
    "bb_ren.expand(1.1)\n",
    "bb_orig.expand(1.1)\n",
    "\n",
    "#add margin around bb for correct ratio:\n",
    "bb_ren.expand_to_correct_ratio(w,h)\n",
    "bb_ren_resized = copy.deepcopy(bb_ren)\n",
    "bb_ren_resized.move(+center_ren[0],+center_ren[1]) #move bb to center of object\n",
    "\n",
    "#reshape the original\n",
    "bb_orig.expand_to_correct_ratio(w,h)\n",
    "bb_orig_resized = copy.deepcopy(bb_orig)\n",
    "bb_orig_resized.move(+center_orig[0],+center_orig[1]) #move bb to center of object\n",
    "\n",
    "bb_orig_resized.plot( img_ori )\n",
    "\n",
    "bb_orig_resized.add_noise(std_h=0, std_w=30)\n",
    "bb_orig_resized.expand_to_correct_ratio(w,h)\n",
    "\n",
    "\n",
    "bb_orig_resized.plot( img_ori )\n",
    "\n",
    "cropped_ori = bb_orig_resized.crop(img_ori)\n",
    "\n",
    "plt_img(cropped_ori)\n",
    "\n",
    "bb_ren_resized.plot(img_r)\n",
    "\n",
    "\n",
    "cropped_r = bb_ren_resized.crop(img_r)\n",
    "\n",
    "plt_img(cropped_r)\n",
    "\n",
    "\n",
    "pil_r = Image.fromarray(cropped_r).resize((640,480))\n",
    "plt_img(np.array(pil_r))\n",
    "np.array(pil_r).shape\n",
    "\n",
    "pil_ori = Image.fromarray(cropped_ori).resize((640,480))\n",
    "plt_img(np.array(pil_ori))\n",
    "np.array(pil_ori).shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_this",
   "language": "python",
   "name": "track_this"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
